{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS342 Machine Learning\n",
    "\n",
    "# Lab 5: PCA for dimensionality reduction & eigenfaces\n",
    "\n",
    "## Department of Computer Science, University of Warwick\n",
    "\n",
    "\n",
    "Recall that Principal Component Analysis (PCA) is a latent factor model that allows finding a new basis, **Z**, where a sample **x**$_{i}$ can be represented as a combination of basic parts, or principal components (PCs). Each **z**$_{i}$ in matrix **Z** is the new feature representation of **x**$_{i}$. If the dimensions of **z**$_{i}$ are less than those of **x**$_{i}$, PCA provides dimensionality reduction. \n",
    "\n",
    "In this lab, we will explore PCA on imaging data. Specifically, we will use the face images contained in the _Labeled Faces in the Wild_ (LFW) dataset for classification after applying PCA. \n",
    "\n",
    "When PCA is applied to face images, the PCs are called **eigenfaces** because they can be reshaped into matrices and displayed as images.\n",
    "\n",
    "The LFW dataset contains grey-level face images of famous people. All images are resized to a size of $47 \\times 62$ pixels. For more information about the dataset, please check this link: http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data files for the lab\n",
    "\n",
    "The data will be loaded directly from the module directory, as shown in the scripts below. Because of the file sizes, these data are not available to be downloaded from the CS342 website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the code below to upload the LFW dataset and to split it into a training set and a test set\n",
    "\n",
    "a. Note that when working with images, they should be re-shaped from matrices to feature vectors.\n",
    "\n",
    "b. File X.npy contains several images, while file y.npy contains the labels (the name of the person displayed in each image). The labels in this lab will be used simply to display the names.\n",
    "\n",
    "b. In this lab, we will only use the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load all the images\n",
    "X = np.load(\"/modules/cs342/2020/lab5/data/X.npy\")\n",
    "y = np.load(\"/modules/cs342/2020/lab5/data/Y.npy\")\n",
    "\n",
    "# Display the number of samples\n",
    "print (\"Total number of images: \",X.shape[0])\n",
    "\n",
    "#Reshape the first image\n",
    "first_image = X[0].reshape(62,47)\n",
    "h , w = first_image.shape\n",
    "\n",
    "# Display the width and height of each image\n",
    "print (\"Image HEIGHT: \",h)\n",
    "print (\"Image WIDTH: \",w)\n",
    "\n",
    "target_names = ['Colin Powell','Donald Rumsfeld','George W Bush','Gerhard Schroeder','Tony Blair']\n",
    "\n",
    "# Display the classes available in the dataset\n",
    "print (\"Classes : %d\" % len(target_names),target_names)\n",
    "\n",
    "# Split all data (X) into a training and testing datasets. The test dataset contains 30% of all data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "n, d = X_train.shape\n",
    "print (\"Number of images in the training dataset: \",n)\n",
    "print (\"Dimensions of feature vectors in the training dataset: \",d)\n",
    "n, d = X_test.shape\n",
    "print (\"Number of images in the test dataset: \",n)\n",
    "print (\"Dimensions of feature vectors in the test dataset: \",d)\n",
    "\n",
    "plt.imshow(first_image, cmap='bone')\n",
    "plt.title(\"FIRST Training Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a number of training images\n",
    "\n",
    "a. Use the _add_subplot_ command to display the **first 15 training images** in a grid of 3 rows X 5 columns. \n",
    "\n",
    "b. Each **figure** should be displayed with a size of 12 X 12. Hint: use parameter _figsize(12,12)_.\n",
    "\n",
    "b. Use the 'bone' color map to display the images.\n",
    "\n",
    "c. Display the label of each image. Hint: use the _set_title_ command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to display the first 15 training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function that computes the PCA decomposition of a dataset using Singular Value Decomposition (SVD)\n",
    "\n",
    "a. Recall that SVD computes three matrices for an input matrix **X**: **U**, **S**, and **V**. \n",
    "\n",
    "b. Your function should use as arguments the dataset to be transformed and the number of top PCs to be used.\n",
    "\n",
    "c. Recall to center the data before computing its SVD.\n",
    "\n",
    "d. The function should return the top PCs to be used.\n",
    "\n",
    "**Hint:** use the _numpy.linalg.svd_ implementation of SVD with _full_matrices=True_. **Note:** this implementation produces a matrix **V** where each row is a PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to compute the PCA decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the mean image and a number of top PCs as computed using your PCA function on the training data\n",
    "\n",
    "a. Recall that to center the data, the mean of each feature should be computed. If all the means are put together in a single matrix, they can be displayed as a single image. This image is called the mean image. Display the **mean image** of the training data with the appropriate label.\n",
    "\n",
    "b. Use the _add_subplot_ command to display the top 15 PCs of the training data as images in a grid of 3 rows X 5 columns. Recall that matrix **V** contains the PCs as the rows of the matrix. The first row corresponds to the top PC.\n",
    "\n",
    "c. Each PC, or eigenface, should be displayed with a **figure** size of 12 X 12. Use the 'bone' color map to display the top 15 PCs.\n",
    "\n",
    "d. Display the component number as the label of each PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display mean image and top PCs (eigenfaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the results of your PCA function to transform the test images using a number of top PCs\n",
    "\n",
    "a. Recall centering the data before transformation by using the mean of the training data. \n",
    "\n",
    "b. Transform the test dataset by using the top {100,50,10,2} PCs of the training data.\n",
    "\n",
    "c. Plot the transformed data in 2D as a scatter plot (use the _scatter_ implementation) and assig a distinct color to each class. The _x_-axis should be the first PC, while the _y_-axis should be the second PC. **Hint:** you may use _np.unique_ to choose the unique labels and _np.where_ to choose the transformed samples based on their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform test images using your implementation of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the transformed test images and display them\n",
    "\n",
    "a. Reconstruct all transformed test images (for the top {100, 50, 10,2} PCs).\n",
    "\n",
    "b. Display the first **5** reconstructed images in a grid of 4 rows X 5 columns with a **figure** size of 12 X 12. Row 1 should display the first **5** images reconstructed by using the top 100 PCs, the second row should display the first **5** images reconstructed by using the top 50 PCs, row 3 should display the first **5** images reconstructed by using the top 10 PCs, and row 4 should display the first **5** images reconstructed by using the top 2 PCs. The total number of images to be displayed should be **20**.\n",
    "\n",
    "c. Use the 'bone' color map to display the reconstructed images.\n",
    "\n",
    "d. Display the label of each reconstructed image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruct the transformed test images and display them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Mean Squared Error (MSE) between all reconstructed test images and the original test images\n",
    "\n",
    "a. The MSE should be computed for the four cases; i.e., reconstruction after transformation using the top {100, 50, 10, 2} PCs. Recall that the MSE measures the average of the squared errors, i.e., the average squared difference between the reconstructed images and the original images. \n",
    "\n",
    "b. Make sure to clearly display the case that achieves the lowest reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Mean Squared Error (MSE) between all reconstructed test images and the original test images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
